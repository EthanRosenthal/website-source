{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1979fc-05a2-487a-a834-16d9eeddfae3",
   "metadata": {},
   "source": [
    "<blockquote class=\"bluesky-embed\" data-bluesky-uri=\"at://did:plc:y5qiqqtzjmlwggzuttldivxq/app.bsky.feed.post/3l7jjvgnodp2f\" data-bluesky-cid=\"bafyreifrve66wcxbsr5smir3exhrkomkaacunadng2ky6vschoewcekl5m\"><p lang=\"en\">As for the ever popular Python vs R vs Julia vs Scala...\n",
    "\n",
    "It&#x27;s already been decided and the right answer is YAML. It ever has ML *in the name*!\n",
    "\n",
    "ðŸ˜‰<br><br><a href=\"https://bsky.app/profile/did:plc:y5qiqqtzjmlwggzuttldivxq/post/3l7jjvgnodp2f?ref_src=embed\">[image or embed]</a></p>&mdash; Alex Gude  (<a href=\"https://bsky.app/profile/did:plc:y5qiqqtzjmlwggzuttldivxq?ref_src=embed\">@alexgude.com</a>) <a href=\"https://bsky.app/profile/did:plc:y5qiqqtzjmlwggzuttldivxq/post/3l7jjvgnodp2f?ref_src=embed\">October 27, 2024 at 5:23 PM</a></blockquote><script async src=\"https://embed.bsky.app/static/embed.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "## Why can't I just write code?\n",
    "\n",
    "I'm coming up on 10 years, and half as many jobs, in data science and machine learning. No matter what, in every role, I find myself reinventing a programming language on top of YAML in order to train machine learning models. \n",
    "\n",
    "In the beginning, there's a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de9d22c-abb1-4b11-a0eb-5971b88ee49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\", parse_dates=[\"timestamp\"])\n",
    "y = df[\"label\"]\n",
    "ts = df[\"timestamp\"]\n",
    "X = df.drop(columns=[\"label\", \"timestamp\"])\n",
    "train_mask = ts < \"2022-01-01\"\n",
    "test_mask = ts >= \"2022-01-01\"\n",
    "\n",
    "X_train, y_train = X[train_mask], y[train_mask]\n",
    "X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "model = LogisticRegression(C=10, random_state=666)\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a2c57-3553-4f2c-aade-d10a7046818a",
   "metadata": {},
   "source": [
    "You find yourself manually modifying parameters in the code, so you move the parameters to a command line interface (CLI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a9f4563-114a-4cb9-8c5e-182e652ad2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typer\n",
    "\n",
    "app = typer.Typer()\n",
    "\n",
    "@app.command()\n",
    "def main(filename: str, start_date: str, end_date: str):\n",
    "    df = pd.read_csv(filename, parse_dates=[\"timestamp\"])\n",
    "    y = df[\"label\"]\n",
    "    ts = df[\"timestamp\"]\n",
    "    X = df.drop(columns=[\"label\", \"timestamp\"])\n",
    "    train_mask = ts < start_date\n",
    "    test_mask = ts >= end_date\n",
    "    \n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "    \n",
    "    model = LogisticRegression(C=10, random_state=666)\n",
    "    model = model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     app()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc8aa7-733b-4c2f-a9ee-b6ce913eece1",
   "metadata": {},
   "source": [
    "But, you find yourself continually adding parameters and hand-designing switch points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce71e17-2edc-412f-93bd-044e3cdc3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def get_model(model_type: str):\n",
    "    if model_type == \"lr\":\n",
    "        return LogisticRegression(C=10, random_state=666)\n",
    "    elif model_type == \"rf\":\n",
    "        return RandomForestClassifier(random_state=666)\n",
    "\n",
    "def get_train_data(train_start_date: str, train_end_date: str):\n",
    "    ...\n",
    "\n",
    "def get_test_data(test_start_date: str, test_end_date: str):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c430fc36-62b4-4141-848e-a7188cfbf30f",
   "metadata": {},
   "source": [
    "You now have too many CLI parameters to hold in your head, and [you can't remember](https://bsky.app/profile/ethanrosenthal.com/post/3lb3qm4vmak22) which ones you used on that run yesterday. So, you get the big brained idea to put all of your parameters into a single configuration file. You pick YAML because it's easier to use than JSON, and you still don't really know what TOML is.\n",
    "\n",
    "Your hand-designed switch points are now actually nodes in a large [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph). You want maximum flexibility, so you push the configuration language to the brink with custom constructors that [dynamically instantiate Python classes](https://matthewpburruss.com/post/yaml/) and [import other yaml files](https://pypi.org/project/pyyaml-include/).\n",
    "\n",
    "You sit back and marvel at the beauty of declarative configuration.\n",
    "\n",
    "```yaml\n",
    "model:\n",
    "  !Load \n",
    "  cls: sklearn.ensemble.RandomForestClassifier\n",
    "  params:\n",
    "    n_estimators: 3000\n",
    "    random_state: 666\n",
    "\n",
    "data: \n",
    "  train: !Include ../data/train_config.yaml\n",
    "  test: !Include ../data/test_config.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557e0d4f-fe5a-4132-a6a2-cc8f0f12455c",
   "metadata": {},
   "source": [
    "You hand off your beautiful configuration framework to the rest of the team, and nobody has any idea what the fuck is going on.\n",
    "\n",
    "## [What happened](https://www.youtube.com/watch?v=0M8zNwh5AgA)?\n",
    "\n",
    "Whoops! It turns out you created a full on programming language without any modern benefits. Global YAML nodes are used across multiple files without any way to track references. Type validation is minimal. The IDE and mypy are unaware of all these python references lurking in YAML. And we end up writing un-pythonic code that's been shoehorned to work with YAML.\n",
    "\n",
    "A couple years ago, I started midwitting the whole thing.\n",
    "\n",
    "{{< figure src=\"/images/yaml-ml/midwit_code.jpg\" >}}\n",
    "\n",
    "It was easy to smugly preach that people should just write code. Just use [pydantic](https://docs.pydantic.dev/latest/) instead of YAML! I even did this in a previous role. \n",
    "\n",
    "Like everything in programming, easier said than done. Somehow, even though everything was type-validated, pydantic-based Python, we still spent way too much time fitting the code into the configuration framework that we'd built. \n",
    "\n",
    "Why can't I just write code? \n",
    "\n",
    "## What is unique to ML?\n",
    "\n",
    "I finally realized why it's hard to just write code. There are 3 primary requirements for an ML config system:\n",
    "\n",
    "### 1. The ability to call different code throughout the DAG.\n",
    "\n",
    "ML is research. It's experimentation. It requires iteration and flexibility.\n",
    "\n",
    "I want to be able to try out different classifiers, different loss functions, different preprocessing methods, etc... The easiest way to support this is by allowing code to be written and called. \n",
    "\n",
    "We don't want to hand-change parameters deep within a nested DAG in order to experiment with things. We'd rather define all of our parameters at the start of the DAG, and then let the DAG code handle the rest.\n",
    "\n",
    "### 2. Lazy-instantiation of classes in the DAG.\n",
    "\n",
    "If I am fine-tuning a pretrained model, I don't want to have to download the entire pretrained model when I define my configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92472a1c-bd06-496d-9605-852b407c03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some quick imports\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import transformers\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "from transformers import AutoModelForSequenceClassification, PreTrainedModel\n",
    "\n",
    "transformers.logging.set_verbosity(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tqdm.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba4ae29-aea7-4289-b3ca-881a28edc440",
   "metadata": {},
   "source": [
    "In the code below, all of BERT gets downloaded when I define my configuration object. You can imagine worse scenarios where instantiating a model requires allocating huge amounts of GPU memory. We should not do this when our config is defined. This is especially bad if we're running a distributed training job and need to wait until we're on a given node before instantiating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa6d7c97-063f-4c7e-9f2f-a9eac9081471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad\n",
    "class TrainingConfig(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    model: PreTrainedModel\n",
    "    \n",
    "\n",
    "config = TrainingConfig(\n",
    "    # The model weights get downloaded when defining the config.\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f082922-b894-435a-accc-3bde559c2eec",
   "metadata": {},
   "source": [
    "So, model instantiation should be deferred as long as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3972ba7-0590-4a12-9736-5d7f5636c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better\n",
    "class TrainingConfig(BaseModel):\n",
    "    model_id: str \n",
    "\n",
    "config = TrainingConfig(model_id=\"bert-base-uncased\")\n",
    "\n",
    "...\n",
    "\n",
    "# Sometime Later\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb5f2c-36dc-4d43-a8b5-554770ba4d7c",
   "metadata": {},
   "source": [
    "### 3. Tracking of all configuration (hyper)parameters\n",
    "\n",
    "We run so many experiments when training ML models that we have to pay for experiment trackers like [Weights & Biases](https://wandb.ai/site/). Crucial to experiment tracking is tracking what changed in between different training runs. The easiest way to do this in a tool like W&B is to put all configuration into a dictionary and log the dict for a given run. \n",
    "\n",
    "It turns out this tracking requirement is at direct odds with \"just writing code\". \n",
    "\n",
    "It's easy to just write the code to instantiate a model in PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "863965cb-a54b-48e8-9cb4-42c3b7ade46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_features: int, \n",
    "        num_classes: int,\n",
    "        hidden_size: int = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_features, self.hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "model = MyModel(1_000, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cde63b-b5dc-463a-aa09-00b42fce3c5e",
   "metadata": {},
   "source": [
    "But, now I can't track the model's constructor arguments. \n",
    "\n",
    "A side benefit of ensuring our configuration is trackable is that we get some semblance of reproducibility, since our configuration must be serializable into something like JSON or YAML. One reason YAML is so popular is that we don't have to serialize anything!\n",
    "\n",
    "## Why can't we separate YAML from ML?\n",
    "\n",
    "YAML solves for all of our requirements. \n",
    "\n",
    "1. We can write custom constructors to call arbitrary code throughout our DAG.\n",
    "2. No python code is instantiated when we instantiate our config\n",
    "3. YAML is easily trackable and reproducible.\n",
    "\n",
    "The problem, of course, is everything that I mentioned at the beginning.\n",
    "\n",
    "\n",
    "## What are the solutions?\n",
    "\n",
    "If you're okay with refactoring your entire codebase, then you should make all your classes take a single configuration object as their arguments. This will solve for all three problems while giving you nice, structured, non-YAML configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8367dac8-75fc-4f7d-99a5-79091005cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "\n",
    "import torch\n",
    "\n",
    "class MyModelConfig(BaseModel):\n",
    "    num_features: int\n",
    "    num_classes: int\n",
    "    hidden_size: int = 128\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, config: MyModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(config.num_features, config.hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(config.hidden_size, config.num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TrainingConfig(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    model_cls: Type[MyModel]\n",
    "    model_cfg: MyModelConfig\n",
    "\n",
    "\n",
    "# No models are instantiated in the config.\n",
    "config = TrainingConfig(\n",
    "    model_cls=MyModel,\n",
    "    model_cfg=MyModelConfig(num_features=100, num_classes=10)\n",
    ")\n",
    "# Lazy-load the model.\n",
    "model = config.model_cls(config.model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93016698-4761-4b76-97fa-8e40df30cc95",
   "metadata": {},
   "source": [
    "You can even try to be a little fancy and define the class configs inside the class, so that they're always coupled to each other. This makes it easy to dynamically instantate the class, based on the class config alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04da92db-7c39-4674-856a-0c6bbeaf3cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "def load_config(config: BaseModel):\n",
    "    module = config.__class__.__module__\n",
    "    parent = config.__class__.__qualname__.split(\".\")[0]\n",
    "    return getattr(importlib.import_module(module), parent)(config)\n",
    "\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    class Config(BaseModel):\n",
    "        num_features: int\n",
    "        num_classes: int\n",
    "        hidden_size: int = 128\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(config.num_features, config.hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(config.hidden_size, config.num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TrainingConfig(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    model_cfg: BaseModel\n",
    "\n",
    "\n",
    "config = TrainingConfig(\n",
    "    model_cfg=MyModel.Config(num_features=100, num_classes=10)\n",
    ")\n",
    "model = load_config(config.model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5329e-5b0b-4a61-b0e8-4909c21f929a",
   "metadata": {},
   "source": [
    "The problem with this approach, of course, is that it requires refactoring your entire codebase. This also become particularly obnoxious since it requires creating a config for _everything_, including classes that you would rather not lazily instantiate. \n",
    "\n",
    "What I find that people often do is they avoid refactoring their codebase by creating separate configs that match their class constructor arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6191809c-cfee-4beb-b25a-01fe62cb03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelConfig(BaseModel):\n",
    "    num_features: int\n",
    "    num_classes: int\n",
    "    hidden_size: int = 128\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_features: int, \n",
    "        num_classes: int,\n",
    "        hidden_size: int = 128\n",
    "    ):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4d297f-a1ea-4045-a78b-0b5f48b5ea1c",
   "metadata": {},
   "source": [
    "The code duplication here is painful.\n",
    "\n",
    "A better solution does 3 things:\n",
    "\n",
    "1. No class constructor refactoring. Call classes as they would be called.\n",
    "1. Track all class constructor arguments in a serializable, centralized place.\n",
    "1. Somehow _also_ allow for lazy-instantation, even though the class constructors are being called.\n",
    "\n",
    "Step 3 could be skipped if you refactor to ensure that all model's constructors are \"lightweight\" and don't involve anything like downloading giant weights, allocating lots of GPU memory, etc... but that places major constraints on your codebase.\n",
    "\n",
    "Does a solution for all 3 steps exist? I haven't seen it yet, but I have some ideas. I just need a little more time to lazy-load them."
   ]
  }
 ],
 "metadata": {
  "hugo": {
   "date": "2024-11-19",
   "draft": false,
   "hasMath": false,
   "notebook": true,
   "slug": "yaml-ml",
   "tags": [
    "data-science",
    "machine-learning-engineering"
   ],
   "title": "Why can't we separate YAML from ML?"
  },
  "kernelspec": {
   "display_name": "yaml-ml",
   "language": "python",
   "name": "yaml-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
